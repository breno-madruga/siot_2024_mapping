{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing the Scopus publications related to the Tiny GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the library.\n",
    "# %pip install pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import re, csv, pandas as pd, numpy as np\n",
    "# from pylatexenc.latex2text import LatexNodes2Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataframe from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the raw data.\n",
    "df_data = pd.read_csv(\"../data/raw/scopus_raw.csv\", header=0, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataframe.\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning and preprocessing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the invalid articles.\n",
    "df_data = df_data.loc[df_data.id.notnull() & df_data.eid.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the \"None\" value for the \"NaN\" values.\n",
    "df_data.replace({np.nan: None}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the \"zero\" value for the articles without numbers of citation and references.\n",
    "df_data.citation_num.loc[df_data.citation_num.isnull()] = 0\n",
    "df_data.ref_count.loc[df_data.ref_count.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"abstract\".\n",
    "df_data.abstract.loc[df_data.abstract.isnull() & df_data.description.notnull()] = df_data.description.loc[\n",
    "    df_data.abstract.isnull() & df_data.description.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"vehicle_name\".\n",
    "df_data.vehicle_name.loc[df_data.conference_name.notnull() & df_data.vehicle_name.notnull()] = df_data.conference_name.loc[\n",
    "    df_data.conference_name.notnull() & df_data.vehicle_name.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary columns.\n",
    "columns_drop = [\"eid\", \"pii\", \"description\", \"isbn\", \"conf_location\", \"conference_name\",\n",
    "    \"vehicle_address\", \"title_edition\", \"pubmed_id\"]\n",
    "df_data.drop(axis=1, columns=columns_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of some features.\n",
    "df_data.loc[:, [\"citation_num\", \"ref_count\"]] = df_data.loc[:, [\"citation_num\", \"ref_count\"]].astype(np.float32)\n",
    "df_data.auth_keywords.loc[df_data.auth_keywords.notnull()] = df_data.auth_keywords.loc[\n",
    "    df_data.auth_keywords.notnull()].apply(eval)\n",
    "df_data.index_terms.loc[df_data.index_terms.notnull()] = df_data.index_terms.loc[\n",
    "    df_data.index_terms.notnull()].apply(eval)\n",
    "df_data.affiliations.loc[df_data.affiliations.notnull()] = df_data.affiliations.loc[\n",
    "    df_data.affiliations.notnull()].apply(eval)\n",
    "df_data.subject_areas.loc[df_data.subject_areas.notnull()] = df_data.subject_areas.loc[\n",
    "    df_data.subject_areas.notnull()].apply(eval)\n",
    "df_data.authors.loc[df_data.authors.notnull()] = df_data.authors.loc[df_data.authors.notnull()].apply(eval)\n",
    "df_data.author_affil.loc[df_data.author_affil.notnull()] = df_data.author_affil.loc[\n",
    "    df_data.author_affil.notnull()].apply(eval)\n",
    "df_data.references.loc[df_data.references.notnull()] = df_data.references.loc[\n",
    "    df_data.references.notnull()].apply(eval)\n",
    "df_data.publication_date = pd.to_datetime(df_data.publication_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the features \"period\" and \"year\" from the feature \"publication_date\".\n",
    "if \"period\" not in df_data:\n",
    "    df_data[\"period\"] = df_data.publication_date.apply(lambda x: \"{}-{}\".format(x.year, x.month))\n",
    "if \"year\" not in df_data:\n",
    "    df_data[\"year\"] = df_data.publication_date.apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the itens contained in the features \"auth_keywords\", \"index_terms\" and \"subject_areas\".\n",
    "df_data.loc[df_data.auth_keywords.notnull(), \"auth_keywords\"] = df_data.loc[\n",
    "    df_data.auth_keywords.notnull(), \"auth_keywords\"].apply(\n",
    "        lambda x: tuple(set([item.strip().lower() for item in x])))\n",
    "df_data.loc[df_data.index_terms.notnull(), \"index_terms\"] = df_data.loc[\n",
    "    df_data.index_terms.notnull(), \"index_terms\"].apply(\n",
    "        lambda x: tuple(set([item.strip().lower() for item in x])))\n",
    "df_data.loc[df_data.subject_areas.notnull(), \"subject_areas\"] = df_data.loc[\n",
    "    df_data.subject_areas.notnull(), \"subject_areas\"].apply(\n",
    "        lambda x: tuple(set([item[\"area\"].strip().lower() for item in x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking there are invalid values in the features \"auth_keywords\", \"index_terms\" and \"subject_areas\".\n",
    "for column in [\"auth_keywords\", \"index_terms\", \"subject_areas\"]:\n",
    "    count = df_data.loc[df_data[column].notnull(), column][\n",
    "                [np.any([item == None or item.lower() == \"none\" for item in items])\n",
    "                 for items in df_data.loc[df_data[column].notnull(), column]]].size\n",
    "    print(\"{}: {}\".format(column, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the invalid values in the features \"auth_keywords\", \"index_terms\" and \"subject_areas\".\n",
    "for column in [\"auth_keywords\", \"index_terms\", \"subject_areas\"]:\n",
    "    df_data.loc[df_data[column].notnull(), column] = [\n",
    "        tuple([item for item in items if item])\n",
    "        for items in df_data.loc[df_data[column].notnull(), column]]\n",
    "    df_data.loc[df_data[column].notnull(), column] = df_data.loc[\n",
    "        df_data[column].notnull(), column].apply(lambda x: x if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the invalid values in the features \"authors\", \"affiliations\" and \"author_affil\".\n",
    "for column in [\"authors\", \"affiliations\", \"author_affil\"]:\n",
    "    df_data.loc[df_data[column].notnull(), column] = df_data.loc[\n",
    "        df_data[column].notnull(), column].apply(lambda x: x if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the affiliations' and authors' IDs for those that have not a ID.\n",
    "df_data.loc[df_data.author_affil.notnull(), \"author_affil\"] = df_data.loc[\n",
    "    df_data.author_affil.notnull(), \"author_affil\"].apply(lambda x: tuple([{\n",
    "        \"id\": item[\"id\"] if item[\"id\"] is not None and item[\"name\"] is not None else \\\n",
    "            str(hash(\"{} - {}\".format(item[\"name\"], \"Scopus\"))) if item[\"name\"] is not None else None,\n",
    "        \"name\": item[\"name\"],\n",
    "        \"affil_id\": item[\"affil_id\"] if item[\"affil_id\"] is not None and item[\"affiliation\"] is not None else \\\n",
    "            str(hash(\"{} - {}\".format(item[\"affiliation\"], \"Scopus\"))) \\\n",
    "                if item[\"affiliation\"] is not None else None,\n",
    "        \"affiliation\": item[\"affiliation\"], \"country\": item[\"country\"]}\n",
    "    for item in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates within the list of affiliations and authors.\n",
    "df_data.loc[df_data.author_affil.notnull(), \"author_affil\"] = [\n",
    "    set([(au[\"id\"], au[\"name\"], au[\"affil_id\"],\n",
    "        au[\"affiliation\"], au[\"country\"]) for au in row])\n",
    "    for row in df_data.loc[df_data.author_affil.notnull(), \"author_affil\"]]\n",
    "df_data.loc[df_data.author_affil.notnull(), \"author_affil\"] = [tuple([dict(zip(\n",
    "        [\"id\", \"name\", \"affil_id\", \"affiliation\", \"country\"], au)) for au in row])\n",
    "    for row in df_data.loc[df_data.author_affil.notnull(), \"author_affil\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicated records by feature \"id\".\n",
    "df_data = df_data.sort_values(by=[\"id\", \"period\"]).drop_duplicates(\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicated records by features \"title\" and \"doi\".\n",
    "df_data = pd.concat([df_data[df_data.title.isnull() | df_data.doi.isnull()],\n",
    "    df_data[df_data.title.notnull() & df_data.doi.notnull()].sort_values(\n",
    "        by=[\"title\", \"citation_num\", \"publication_date\"]).drop_duplicates(\n",
    "            subset=[\"title\", \"doi\"], keep=\"last\")], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the result.\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the data to CSV file.\n",
    "df_data.to_csv(\"../data/prepared/scopus_tiny_genai.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50bd5474255f82aa829301912ce59e29110123be660cf8d7583f66a20371684"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
