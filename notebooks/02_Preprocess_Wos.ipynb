{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing the WoS publications related to the Tiny GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the library.\n",
    "# %pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import re, csv, pandas as pd, numpy as np\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataframe from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the raw data.\n",
    "df_data = pd.read_csv(\"../data/raw/wos_raw.csv\", header=0, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataframe.\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning and preprocessing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_affiliations(row):\n",
    "    fix_values = {\"Electronics & Telecommunications Research Institute - Korea (ETRI)\": \"Electronics & Telecommunications Research Institute\",\n",
    "                  \"ETRI, Daejeon, South Korea\": \"Electronics & Telecommunications Research Institute, Daejeon, South Korea\",\n",
    "                  \"Binzhou Univ\": \"Shandong University of Aeronautics\", \"Utah System of Higher Education\": \"University of Utah\",\n",
    "                  \"NYU, New York, NY USA\": \"New York University, New York, NY USA\",# \"Univ Rostock\": \"University of Rostock\",\n",
    "                  \"Symbiosis Centre for Information Technology (SCIT)\": \"Symbiosis International University\",\n",
    "                  \"Univ Hlth Network\": \"University of Toronto\", \"University Health Network Toronto\": \"University of Toronto\",\n",
    "                  \"Harvard Med Sch\": \"Harvard University\", \"Harvard Medical School\": \"Harvard University\",\n",
    "                  \"MIT, Dept Elect Engn & Comp Sci, Cambridge, MA USA\": \"Massachusetts Institute of Technology, Dept Elect Engn & Comp Sci, Cambridge, MA USA\",\n",
    "                  \"Massachusetts Gen Hosp\": \"Massachusetts General Hospital\", \"University of California System\": \"University of California San Francisco\",\n",
    "                  \"Helmholtz Zentrum Munchen\": \"Helmholtz Association\", \"Helmholtz-Center Munich - German Research Center for Environmental Health\": \"Helmholtz Association\"}\n",
    "    for k, v in fix_values.items():\n",
    "        row.affiliations = row.affiliations.replace(k, v)\n",
    "        row.author_affil = row.author_affil.replace(k, v)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_authors(row):\n",
    "    records = list()\n",
    "    for auth in row.authors.split(\";\"):\n",
    "        auth = [a.strip() for a in auth.strip().split(\",\")]\n",
    "        auth = f\"{auth[1]} {auth[0]}\"\n",
    "        record = {\"name\": auth}\n",
    "        if row.researcher_ids is not None:\n",
    "            for id_auth in row.researcher_ids.split(\";\"):\n",
    "                id_auth = id_auth.strip().split(\"/\")\n",
    "                id_auth[0] = [a.strip() for a in id_auth[0].split(\",\")]\n",
    "                if (\" \".join(id_auth[0]).lower().startswith(record[\"name\"].lower()) or \" \".join(id_auth[0][::-1]).lower().startswith(record[\"name\"].lower())) and \"id\" not in record:\n",
    "                    record[\"id\"] = id_auth[-1].strip()\n",
    "        elif row.orcids is not None:\n",
    "            for id_auth in row.orcids.split(\";\"):\n",
    "                id_auth = id_auth.strip().split(\"/\")\n",
    "                id_auth[0] = [a.strip() for a in id_auth[0].split(\",\")]\n",
    "                if (\" \".join(id_auth[0]).lower().startswith(record[\"name\"].lower()) or \" \".join(id_auth[0][::-1]).lower().startswith(record[\"name\"].lower())) and \"id\" not in record:\n",
    "                    record[\"id\"] = id_auth[-1].strip()\n",
    "        if \"id\" not in record:\n",
    "            record[\"id\"] = str(hash(\"{} - {}\".format(record[\"name\"], \"Web of Science\")))\n",
    "        records.append(record)\n",
    "    return tuple(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_affiliations(row):\n",
    "    records = list()\n",
    "    if row.affiliations != row.author_affil:\n",
    "        # Fixing the textual content and removing affiliations duplicated.\n",
    "        row.author_affil = re.sub(r\"\\[[^\\]]+\\]\", \"\", row.author_affil)\n",
    "        row.affiliations = list(set([re.sub(r\"\\s+\", \" \", affil.lower()).strip()\n",
    "                                     for affil in row.affiliations.split(\";\")]))\n",
    "        row.author_affil = list(set([re.sub(r\"\\s+\", \" \", auth.lower()).strip()\n",
    "                                     for auth in row.author_affil.split(\";\")]))\n",
    "        addr_used = list()\n",
    "        for affil in row.affiliations:\n",
    "            items = dict()\n",
    "            for auth in row.author_affil:\n",
    "                item = {\"author_affil\": auth}\n",
    "                auth = [a.strip() for a in auth.split(\",\")]\n",
    "                item = {**item, \"name\": auth[0], \"country\": auth[-1]}\n",
    "                items[fuzz.partial_token_sort_ratio(affil, item[\"name\"])] = item\n",
    "            if items[max(items.keys())][\"author_affil\"] not in addr_used:\n",
    "                record = {\"name\": affil, \"author_affil\": items[max(items.keys())][\"author_affil\"],\n",
    "                          \"country\": items[max(items.keys())][\"country\"]}\n",
    "                addr_used.append(items[max(items.keys())][\"author_affil\"])\n",
    "            else:\n",
    "                record = {\"name\": affil, \"author_affil\": None, \"country\": None}\n",
    "            records.append(record)\n",
    "        if len(set(row.author_affil).difference(set(addr_used))) > 0:\n",
    "            for auth in set(row.author_affil).difference(set(addr_used)):\n",
    "                auth = [a.strip() for a in auth.split(\",\")]\n",
    "                if not np.any([auth[0] == a.split(\",\")[0].strip() for a in addr_used]):\n",
    "                    records.append({\"name\": auth[0], \"author_affil\": None, \"country\": auth[-1]})\n",
    "    else:\n",
    "        row.affiliations = re.sub(r\"\\[[^\\]]+\\]\", \"\", row.affiliations)\n",
    "        row.affiliations = list(set([re.sub(r\"\\s+\", \" \", affil.lower()).strip()\n",
    "                                     for affil in row.affiliations.split(\";\")]))\n",
    "        for auth in row.affiliations:\n",
    "                record = auth.split(\",\")\n",
    "                record = {\"name\": record[0].strip(), \"country\": record[-1].strip()}\n",
    "                records.append(record)\n",
    "    records = [{\"id\": str(hash(\"{} - {}\".format(record[\"name\"], \"Web of Science\"))),\n",
    "                \"affiliation\": record[\"name\"].title(), \"country\": record[\"country\"].title() \\\n",
    "                    if len(set([\"usa\", \"china\"]).intersection(set(record[\"country\"].split()))) == 0 \\\n",
    "                        else \"USA\" if \"usa\" in record[\"country\"].split() else \"China\"\n",
    "                } for record in records]\n",
    "    return tuple(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns.\n",
    "df_data.columns = [re.sub(\"[(),]\", \"\", c.replace(\" \", \"_\").lower()) for c in df_data.columns]\n",
    "cols = {\"180_day_usage_count\": \"day_180_usage_count\", \"publication_year\": \"year\", \"research_areas\": \"subject_areas\",\n",
    "        \"ut_unique_wos_id\": \"id\", \"article_title\": \"title\", \"publication_type\": \"source_type\",\n",
    "        \"document_type\": \"production_type\", \"source_title\": \"vehicle_name\", \"cited_reference_count\": \"ref_count\",\n",
    "        \"author_keywords\": \"auth_keywords\", \"addresses\": \"author_affil\", \"times_cited_wos_core\": \"citation_num\"}\n",
    "df_data.rename(columns=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the \"None\" value for the \"NaN\" values.\n",
    "df_data.replace({np.nan: None}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the \"zero\" value for the articles without numbers of citation and references.\n",
    "df_data.citation_num.loc[df_data.citation_num.isnull()] = 0\n",
    "df_data.ref_count.loc[df_data.ref_count.isnull()] = 0\n",
    "df_data.loc[:, [\"citation_num\", \"ref_count\"]] = df_data.loc[:, [\"citation_num\", \"ref_count\"]].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"vehicle_name\".\n",
    "df_data.vehicle_name.loc[df_data.conference_title.notnull()] = df_data.conference_title.loc[df_data.conference_title.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"publication_date\".\n",
    "df_data.publication_date.loc[df_data.publication_date.notnull()] = df_data.loc[\n",
    "    df_data.publication_date.notnull(), [\"publication_date\", \"year\"]].apply(lambda x: f\"{x.year} {x.publication_date}\".title() \\\n",
    "        if x.year not in x.publication_date else x.publication_date.title(), axis=1)\n",
    "df_data.publication_date.loc[df_data.publication_date.isnull()] = df_data.year.loc[df_data.publication_date.isnull()]\n",
    "df_data.publication_date = df_data.publication_date.apply(lambda x: f\"{x} Jan 01\" if len(x.split()) == 1 else \\\n",
    "    f\"{x} 01\" if len(x.split()) == 2 else x)\n",
    "df_data.publication_date = pd.to_datetime(df_data.publication_date, format=\"%Y %b %d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature \"period\" from the feature \"publication_date\".\n",
    "if \"period\" not in df_data:\n",
    "    df_data.loc[:, \"period\"] = df_data.publication_date.apply(lambda x: \"{}-{}\".format(x.year, x.month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"issn\".\n",
    "df_data.issn = df_data.loc[:, [\"issn\", \"eissn\"]].apply(\n",
    "    lambda x: f\"{x.issn};{x.eissn}\" if x.issn is not None and x.eissn is not None \\\n",
    "        else x.issn if x.issn is not None and x.eissn is None \\\n",
    "            else x.eissn if x.issn is None and x.eissn is not None \\\n",
    "                else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"auth_keywords\".\n",
    "df_data.loc[df_data.keywords_plus.notnull(), \"auth_keywords\"] = df_data.loc[\n",
    "    df_data.keywords_plus.notnull(), [\"auth_keywords\", \"keywords_plus\"]].apply(\n",
    "        lambda x: f\"{x.auth_keywords}; {x.keywords_plus}\", axis=1)\n",
    "df_data.auth_keywords = df_data.auth_keywords.apply(\n",
    "    lambda x: tuple(set([au.strip().lower() for au in x.split(\";\")])) \\\n",
    "        if x is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"subject_areas\".\n",
    "sa = [\"multidisciplinary\", \"computer science\", \"manufacturing\", \"finance\", \"radiology\"]\n",
    "df_data.subject_areas = df_data.loc[:, [\"wos_categories\", \"subject_areas\"]].apply(\n",
    "    lambda x: f\"{x.wos_categories.lower()}; {x.subject_areas.lower()}\", axis=1)\n",
    "df_data.subject_areas = df_data.subject_areas.apply(\n",
    "    lambda x: tuple(set([f\"{field.strip().split(',')[-1].strip()} {field.strip().split(',')[0].strip()}\" \\\n",
    "        if len(field.strip().split(\",\")) > 1 and field.strip().split(',')[0].strip() not in sa \\\n",
    "            else sub_sa.strip() if len(field.strip().split(\",\")) > 1 else field.strip()\n",
    "        for field in x.split(\";\") for sub_sa in field.strip().split(\",\")])) if x is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking there are invalid values in the features \"auth_keywords\" and \"subject_areas\".\n",
    "for column in [\"auth_keywords\", \"subject_areas\"]:\n",
    "    count = df_data.loc[df_data[column].notnull(), column][\n",
    "                [np.any([item == None or item.lower() == \"none\" for item in items])\n",
    "                 for items in df_data.loc[df_data[column].notnull(), column]]].size\n",
    "    print(\"{}: {}\".format(column, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the invalid values in the features \"auth_keywords\" and \"subject_areas\".\n",
    "for column in [\"auth_keywords\", \"subject_areas\"]:\n",
    "    df_data.loc[df_data[column].notnull(), column] = [\n",
    "        tuple([item for item in items if item])\n",
    "        for items in df_data.loc[df_data[column].notnull(), column]]\n",
    "    df_data.loc[df_data[column].notnull(), column] = df_data.loc[\n",
    "        df_data[column].notnull(), column].apply(lambda x: x if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"authors\".\n",
    "df_data.author_full_names.loc[(df_data.author_full_names.isnull()) & (df_data.authors.notnull())] = df_data.authors.loc[\n",
    "    (df_data.author_full_names.isnull()) & (df_data.authors.notnull())]\n",
    "df_data.authors = df_data.author_full_names.copy()\n",
    "df_data.loc[:, \"authors\"] = df_data.loc[:, [\"authors\", \"researcher_ids\", \"orcids\"]].apply(normalize_authors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature \"affiliations\".\n",
    "df_data.affiliations.loc[(df_data.affiliations.isnull()) & (df_data.author_affil.notnull())] = df_data.author_affil.loc[\n",
    "    (df_data.affiliations.isnull()) & (df_data.author_affil.notnull())]\n",
    "df_data.loc[:, [\"affiliations\", \"author_affil\"]] = df_data.loc[:, [\"affiliations\", \"author_affil\"]].apply(fix_affiliations, axis=1)\n",
    "df_data.loc[:, \"affiliations\"] = df_data.loc[:, [\"author_affil\", \"affiliations\"]].apply(normalize_affiliations, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary columns.\n",
    "columns_drop = [\"author_full_names\", \"book_authors\", \"book_editors\", \"book_group_authors\", \"book_author_full_names\", \"group_authors\",\n",
    "                \"book_series_title\", \"book_series_subtitle\", \"conference_title\", \"conference_date\", \"conference_location\",\n",
    "                \"conference_sponsor\", \"conference_host\", \"keywords_plus\", \"reprint_addresses\", \"email_addresses\", \"researcher_ids\", \"orcids\", \"funding_orgs\",\n",
    "                \"funding_name_preferred\", \"funding_text\", \"cited_references\", \"publisher_city\", \"publisher_address\", \"eissn\", \"isbn\",\n",
    "                \"journal_abbreviation\", \"journal_iso_abbreviation\", \"volume\", \"issue\", \"part_number\", \"supplement\",\n",
    "                \"special_issue\", \"meeting_abstract\", \"start_page\", \"end_page\", \"article_number\", \"doi_link\", \"book_doi\",\n",
    "                \"early_access_date\", \"number_of_pages\", \"wos_categories\", \"web_of_science_index\", \"ids_number\", \"pubmed_id\",\n",
    "                \"open_access_designations\", \"highly_cited_status\", \"hot_paper_status\", \"date_of_export\", \"web_of_science_record\",\n",
    "                \"day_180_usage_count\", \"since_2013_usage_count\", \"times_cited_all_databases\"]\n",
    "df_data.drop(axis=1, columns=columns_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the invalid values in the features \"authors\" and \"affiliations\".\n",
    "for column in [\"authors\", \"affiliations\"]:\n",
    "    df_data.loc[df_data[column].notnull(), column] = df_data.loc[\n",
    "        df_data[column].notnull(), column].apply(lambda x: x if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicated records by feature \"id\".\n",
    "df_data = df_data.sort_values(by=[\"id\", \"period\"]).drop_duplicates(\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the result.\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the data to CSV file.\n",
    "df_data.to_csv(\"../data/prepared/wos_tiny_genai.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50bd5474255f82aa829301912ce59e29110123be660cf8d7583f66a20371684"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
